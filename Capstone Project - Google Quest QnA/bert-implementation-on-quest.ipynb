{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Bert Implementation on Quest"},{"metadata":{},"cell_type":"markdown","source":"### Importing The Required Libraries"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"},{"output_type":"stream","text":"2.3.1\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\n\nBERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 384\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","execution_count":2,"outputs":[{"output_type":"stream","text":"train shape = (6079, 41)\ntest shape = (476, 11)\n\noutput categories:\n\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n\ninput categories:\n\t ['question_title', 'question_body', 'answer']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Create model\n\n`compute_spearmanr()` is used to compute the competition metric for the validation set\n<br><br>\n`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() \n    config.output_hidden_states = False \n    \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Obtain inputs and targets, as well as the indices of the train/validation splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1887e6386f9e4740b803f1b1360487a8"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b4d53deab042c8b47c0bf9c237672f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n\nvalid_preds = []\ntest_preds = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 2 folds (out of 5) to manage < 2h\n    if fold in [0, 2]:\n\n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n        \n        K.clear_session()\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        model.fit(train_inputs, train_outputs, epochs=3, batch_size=6)\n        # model.save_weights(f'bert-{fold}.h5')\n        valid_preds.append(model.predict(valid_inputs))\n        test_preds.append(model.predict(test_inputs))\n        \n        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n        print('validation score = ', rho_val)","execution_count":6,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n811/811 [==============================] - 483s 595ms/step - loss: 0.4020\nEpoch 2/3\n811/811 [==============================] - 484s 596ms/step - loss: 0.3685\nEpoch 3/3\n811/811 [==============================] - 484s 596ms/step - loss: 0.3549\nvalidation score =  0.3873238360765599\nEpoch 1/3\n811/811 [==============================] - 482s 595ms/step - loss: 0.3974\nEpoch 2/3\n811/811 [==============================] - 481s 594ms/step - loss: 0.3680\nEpoch 3/3\n811/811 [==============================] - 481s 593ms/step - loss: 0.3536\nvalidation score =  0.3994787522858457\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 6. Process and submit test predictions\n\nAverage fold predictions, then save as `submission.csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n\ndf_sub.to_csv('submission_manav.csv', index=False)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"   qa_id  question_asker_intent_understanding  question_body_critical  \\\n0     39                             0.952785                0.760290   \n1     46                             0.881958                0.565946   \n2     70                             0.930146                0.773189   \n3    132                             0.892254                0.459137   \n4    200                             0.920275                0.455987   \n\n   question_conversational  question_expect_short_answer  \\\n0                 0.173122                      0.464303   \n1                 0.007506                      0.686661   \n2                 0.013499                      0.794085   \n3                 0.011306                      0.712880   \n4                 0.026050                      0.847929   \n\n   question_fact_seeking  question_has_commonly_accepted_answer  \\\n0               0.662660                               0.562189   \n1               0.813135                               0.885134   \n2               0.928641                               0.945611   \n3               0.760518                               0.909132   \n4               0.859432                               0.862676   \n\n   question_interestingness_others  question_interestingness_self  \\\n0                         0.742258                       0.698166   \n1                         0.532006                       0.513669   \n2                         0.666111                       0.545935   \n3                         0.574690                       0.422950   \n4                         0.647015                       0.598715   \n\n   question_multi_intent  ...  question_well_written  answer_helpful  \\\n0               0.471454  ...               0.938026        0.911407   \n1               0.077292  ...               0.706068        0.943407   \n2               0.169583  ...               0.878278        0.912420   \n3               0.134010  ...               0.675856        0.924341   \n4               0.194887  ...               0.620836        0.892824   \n\n   answer_level_of_information  answer_plausible  answer_relevance  \\\n0                     0.472441          0.975754          0.949858   \n1                     0.626379          0.966499          0.977226   \n2                     0.525905          0.966150          0.968138   \n3                     0.663449          0.959232          0.973753   \n4                     0.609849          0.955093          0.957769   \n\n   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n0             0.789397                  0.016338               0.030588   \n1             0.863675                  0.924245               0.140422   \n2             0.860190                  0.026593               0.056660   \n3             0.856740                  0.771155               0.172035   \n4             0.855903                  0.126602               0.084948   \n\n   answer_type_reason_explanation  answer_well_written  \n0                        0.830736             0.889026  \n1                        0.074960             0.897121  \n2                        0.862095             0.918187  \n3                        0.594616             0.889783  \n4                        0.574312             0.891461  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qa_id</th>\n      <th>question_asker_intent_understanding</th>\n      <th>question_body_critical</th>\n      <th>question_conversational</th>\n      <th>question_expect_short_answer</th>\n      <th>question_fact_seeking</th>\n      <th>question_has_commonly_accepted_answer</th>\n      <th>question_interestingness_others</th>\n      <th>question_interestingness_self</th>\n      <th>question_multi_intent</th>\n      <th>...</th>\n      <th>question_well_written</th>\n      <th>answer_helpful</th>\n      <th>answer_level_of_information</th>\n      <th>answer_plausible</th>\n      <th>answer_relevance</th>\n      <th>answer_satisfaction</th>\n      <th>answer_type_instructions</th>\n      <th>answer_type_procedure</th>\n      <th>answer_type_reason_explanation</th>\n      <th>answer_well_written</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>0.952785</td>\n      <td>0.760290</td>\n      <td>0.173122</td>\n      <td>0.464303</td>\n      <td>0.662660</td>\n      <td>0.562189</td>\n      <td>0.742258</td>\n      <td>0.698166</td>\n      <td>0.471454</td>\n      <td>...</td>\n      <td>0.938026</td>\n      <td>0.911407</td>\n      <td>0.472441</td>\n      <td>0.975754</td>\n      <td>0.949858</td>\n      <td>0.789397</td>\n      <td>0.016338</td>\n      <td>0.030588</td>\n      <td>0.830736</td>\n      <td>0.889026</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>46</td>\n      <td>0.881958</td>\n      <td>0.565946</td>\n      <td>0.007506</td>\n      <td>0.686661</td>\n      <td>0.813135</td>\n      <td>0.885134</td>\n      <td>0.532006</td>\n      <td>0.513669</td>\n      <td>0.077292</td>\n      <td>...</td>\n      <td>0.706068</td>\n      <td>0.943407</td>\n      <td>0.626379</td>\n      <td>0.966499</td>\n      <td>0.977226</td>\n      <td>0.863675</td>\n      <td>0.924245</td>\n      <td>0.140422</td>\n      <td>0.074960</td>\n      <td>0.897121</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>0.930146</td>\n      <td>0.773189</td>\n      <td>0.013499</td>\n      <td>0.794085</td>\n      <td>0.928641</td>\n      <td>0.945611</td>\n      <td>0.666111</td>\n      <td>0.545935</td>\n      <td>0.169583</td>\n      <td>...</td>\n      <td>0.878278</td>\n      <td>0.912420</td>\n      <td>0.525905</td>\n      <td>0.966150</td>\n      <td>0.968138</td>\n      <td>0.860190</td>\n      <td>0.026593</td>\n      <td>0.056660</td>\n      <td>0.862095</td>\n      <td>0.918187</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>132</td>\n      <td>0.892254</td>\n      <td>0.459137</td>\n      <td>0.011306</td>\n      <td>0.712880</td>\n      <td>0.760518</td>\n      <td>0.909132</td>\n      <td>0.574690</td>\n      <td>0.422950</td>\n      <td>0.134010</td>\n      <td>...</td>\n      <td>0.675856</td>\n      <td>0.924341</td>\n      <td>0.663449</td>\n      <td>0.959232</td>\n      <td>0.973753</td>\n      <td>0.856740</td>\n      <td>0.771155</td>\n      <td>0.172035</td>\n      <td>0.594616</td>\n      <td>0.889783</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200</td>\n      <td>0.920275</td>\n      <td>0.455987</td>\n      <td>0.026050</td>\n      <td>0.847929</td>\n      <td>0.859432</td>\n      <td>0.862676</td>\n      <td>0.647015</td>\n      <td>0.598715</td>\n      <td>0.194887</td>\n      <td>...</td>\n      <td>0.620836</td>\n      <td>0.892824</td>\n      <td>0.609849</td>\n      <td>0.955093</td>\n      <td>0.957769</td>\n      <td>0.855903</td>\n      <td>0.126602</td>\n      <td>0.084948</td>\n      <td>0.574312</td>\n      <td>0.891461</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}